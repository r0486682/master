\chapter{Conclusion}
\label{ch:conclusion}


\section{Revisiting the problem statement}
The on-demand available compute resources offered by cloud providers combined with the fine-grained resource allocation of container technology  allows SaaS providers to minimize their operating costs. In addition, SaaS providers often employ a multi-tenant architecture in which resources are shared among multiple tenants. The work of \cite{TruyenEddy2016Taca} shows how the resource allocation policies of Kubernetes (e.g., CPU request and limits) can be used to offer a different quality of service to different tenants.  SaaS providers  wish to restrict the capacity of their infrastructure to the amount required to meet the SLOs agreed upon with their customers in signed SLAs. The task of  mapping SLOs to a cost-efficient resource allocation is a problem, referred to as SLA-decomposition. SLA-decomposition is difficult especially for multi-tenant SaaS applications with different performance SLOs for different classes of tenants.
\\\\
Existing approaches for SLA-decomposition of multi-tenant SaaS applications require extensive domain expertise and struggle with the conceptual gap between the model of the application and the true performance characteristics of the application. Moreover,   the required amount of resources for different sizes of the demanded workload volume is inherently a non-linear resource scaling problem. A model accurate for five tenants could be completely wrong for ten tenants.
\section{Summary of contributions}
This thesis has proposed an approach that does not require any domain expertise and that can be applied for different workload sizes in order to estimate the required set of resources for any desired combinations of tenants from various SLA classes. In summary, the thesis makes the following contributions.\\\\
An extensive analysis of the problems surrounding the SLA-decomposition task is presented. The increasing complexity of software systems and their deployment environments results in complex performance characteristics making it impossible for system administrators to correctly estimate required resources. Both related research and conducted experiments show that most systems do not scale linearly and the use of simple scaling policies are thus not economically beneficial.
\\\\
A survey of existing state-of-art approaches related to the problem is provided. Auto-tuning approaches based on continuous experimentation are selected as the most suitable over other model-based approaches. The black box approach of these methods offers more adaptivity for different types of applications and does not demand extensive knowledge of the application. By evaluation through continuous experimentation in the production environment, uncertainty on the quality of the resulting configuration is eliminated. 
\\\\
A specific auto-tuning approach, BestConfig \cite{zhu2017bestconfig}, is applied and evaluated in the context of a simple batch processing application. Adapted for the SLA-decomposition problem it has shown to be capable of finding a cost-efficient resource allocation even in the presence of multiple search parameters. 
\\\\
A prototype has been developed as an easy-to-use tool, named k8-resource optimizer, that integrates well with Kubernetes, the de-facto standard for container orchestration platforms, and a popular package manager for Kubernetes named Helm. Its operational effort for system administrators is limited to the declarative specification of  SLOs, resource search space parameters and workload profiles.
\section{Lessons learned}
The experimental findings have shown that k8-resource-optimizer is capable of producing  optimal or near-optimal SLA-decompositions for a Kubernetes application in deployment scenarios with one or more SLOs for one or more tenant organizations. Whilst further evaluation with different applications is needed, k8-resource-optimizer seems to be an ideal match for contemporary DevOps applications. K8-resource-optimizer can be ideally performed in the production environment when a new version of the application is being dark launched, canary or A/B tested. In addition, the decompositions produced by k8-resource-optimizer could be used in a capacity management policy of a SLA-manager responsible for reactive provisioning and deprovisioning containers whenever a tenant submits a new batch or whenever a batch is finished.
\section{Limitations of k8-resource-optimizer}
The chosen solutions present in the design of k8-resource-optimizer impose several limitations on the capability of the tool. A limitation is defined as an inherent limitation that cannot be envisioned to be solved in future work. The tool is known to have the following limitations.\\\\
While capable of estimating the required resources of a periodic job, the approach is not capable of dealing with SLOs containing deadlines. This task requires the presence of scheduling component such as the online packing algorithm proposed by \cite{jyothi2016morpheus} or the task re-prioritization service presented in \cite{Walraven2015b}.
\\\\
The k8-resource-optimizer's resource regulation capabilities are restricted to resource policies offered by Kubernetes. Currently, Kubernetes resource management concepts to do not support network or disk I/O bandwidth. As a result, k8-resource-optimizer only works out of the box for CPU and memory allocations of the state-less web-tier. \\\\ 
The regulation of network bandwidth for Kubernetes applications requires an external rate limiter at the boundary of the network. The rate limiter limits the presence of network congestion thereby allowing the assumption that the technical specified bandwidth of network adapter is always available. Network traffic control plugins are available for the Mesos orchestration platform~\footnote{\url{https://github.com/apache/mesos/blob/1.5.x/docs/isolators/cgroups-net-cls.md}}. Parameters of these plugins can be auto-tuned. \\\\
In contrast to disk space, disk I/O bandwidth is not controllable per container in Kubernetes. This problem only aggravates in cloud-based environments due to the distributed nature of the storage system underlying persistent services.     However, for multi-tenant SaaS this does not matter. For reasons of simplicity, this argument is defended for traditional web applications where a typical SaaS application is stored as a stateless web tier backed up by a multi-tenant database. In this scenario, the database tier requires a separate SLO management method. Currently, a request scheduler is still the best approach.


\section{Future work}
 A first extension is the further evaluation of the tool with different applications. These applications could require tuning of different Kubernetes configuration parameters such as memory request/limits or the tuning of both CPU and memory allocation simultaneously. Other evaluation could include experimentation where multiple SLO types must be met simultaneously (e.g., throughput and job completion deadlines).
\\\\
A second extension could include optimizations to reduce the runtime of the tool.  The majority of the runtime is spent during the load tests evaluating the performance of a configuration. The runtime of a single load test is decided by the performance of the configuration and the required number of requests to be completed. Weak performing configuration will thus increase the total runtime of the tool. A possible extension could include the specification of a runtime for a single load test instead of the number of requests. Resulting in a fixed specifiable total runtime.
\\\\
A third extension could include the implementation of an SLA-decomposition technique similar to the Bayesian optimization approach used by CherryPick \cite{alipourfard2017cherrypick} discussed in Section \ref{rw:cherry}. A comparison of the performance of both approaches could be conducted. 
% This thesis presented k8-resource-optimizer, an automated tool for the SLA- decomposition of Kubernetes applications. The tool implements an adapted version of the search-based performance optimization algorithm used in Bestconfig \cite{zhu2017bestconfig}, a configuration auto-tuner, to perform a SLA-decomposition. The search-based algorithm treats the application as a black-box making it more adaptable to various applications compared to other model-based approaches. Treating the application as a black-box allows the system administrator to have limited knowledge of the application. The implementation of the tool only requires the specification of the parameters to be optimized and their corresponding search spaces (e.g., CPU limit and request of component X between 200 Millicores and 500 Millicores). The implemented decomposition technique thus fulfills the first goal set in Section \ref{goals}.

% \noindent The design of k8-resource-optimizer includes an interface for Helm, a popular Kubernetes package. Within Helm, Kubernetes applications are specified as templates, referred to as charts, of which settings can be easily configured. By leveraging the concepts of Helm, every Kubernetes application specified as a Helm chart can make use of k8-resource-optimizer. Allowing the tool to be part of an automated production environment and achieving the second goal of Section \ref{goals}.
% \\\\
% \noindent For the evaluation of k8-resource-optimizer, a controlled batch-driven application was developed. The application was specifically designed to  stress the CPU resource and was inspired by the work of \cite{matthews2007quantifying}. This allowed the tool to be evaluated in an environment where quality of service differentiation for tenants is achieved by regulating the allocated CPU resource. Using this controlled application, k8-resource-optimizer was evaluated in various experiments.  The experiments show that k8-resource-optimizer is capable of producing  acceptable SLA-decompositions for a Kubernetes application deployment in settings with multiple tenants from different SLA-classes and multiple Kubernetes configuration parameters (i.e., CPU resource request/limit and replicas). 
% \\\\